# Adding a Playground for Tool Calling Experiments

This describes how to add an Xcode playground to the LilDoc project for prototyping Foundation Models tool definitions and prompt instructions before wiring them into the app.

## Why a playground

The feedback loop for experimenting with tool calling is slow if you have to build and run the full app every time. A playground lets you:

- Define a tool, run it, see the output immediately
- Test different prompt instructions and see how the model selects tools
- Iterate on `@Generable` argument structs without a full build cycle
- Verify tool output is concise enough for the 4,096-token context window

The playground is a scratch pad. Nothing in it ships. Once a tool definition works well, you move it into the app.

## Setup

### 1. Create the playground

In Xcode with the LilDoc project open:

1. **File > New > Playground...**
2. Choose **macOS > Blank** template
3. Name it `ToolExperiments`
4. In the file picker, save it at the project root: `LilDoc/ToolExperiments.playground`
5. When asked "Add to", select the **LilDoc project** and **LilDoc group**

This creates `ToolExperiments.playground` at the top level of the project navigator.

### 2. Verify it can import Foundation Models

Replace the default playground contents with:

```swift
import FoundationModels

let model = SystemLanguageModel.default
print("Model availability: \(model.availability)")
```

Run the playground (Editor > Run Playground or the play button). You should see `available` if Apple Intelligence is enabled on your Mac. If you see `unavailable`, check System Settings > Apple Intelligence & Siri.

### 3. Use playground Sources for reusable code

The playground has a `Sources/` folder (visible in the project navigator when the playground is expanded). Put shared helpers here so the main playground page stays focused on experiments.

Create `Sources/SampleText.swift`:

```swift
import Foundation

/// Sample documents for testing tools without reading real files.
public enum SampleText {
    public static let meetingNotes = """
    # Q1 Planning Meeting

    TODO: Finalize budget by Friday
    TODO: Send hiring plan to VP

    We discussed the product roadmap for Q1. The main priorities are:
    1. Ship the new onboarding flow
    2. Fix the search performance regression
    3. FIXME: The analytics dashboard still shows stale data

    Action items:
    - Alice: draft the onboarding spec
    - Bob: profile the search queries
    - NOTE: We need to revisit the pricing model in February

    HACK: The export feature uses a hardcoded path for now.
    """

    public static let shortNote = """
    Pick up milk.
    Call the dentist.
    Finish the report.
    """
}
```

Files in `Sources/` must use `public` access control to be visible from the playground page.

## Experimenting with tools

### Minimal tool definition

On the main playground page, define and test a tool:

```swift
import FoundationModels

// A self-contained tool that operates on a string (no file I/O needed)
struct CountPatternTool: Tool {
    let name = "countPattern"
    let description = "Count how many times a word or phrase appears in the text."

    // The text to operate on — injected at init, not generated by the model
    let text: String

    @Generable
    struct Arguments {
        @Guide(description: "The word or phrase to count")
        var pattern: String
    }

    func call(arguments: Arguments) async throws -> String {
        let count = text.components(separatedBy: arguments.pattern).count - 1
        return "\(arguments.pattern): \(count) occurrences"
    }
}
```

### Testing a tool by hand

You can call a tool directly without going through the model:

```swift
let tool = CountPatternTool(text: SampleText.meetingNotes)
let result = try await tool.call(arguments: .init(pattern: "TODO"))
print(result) // "TODO: 2 occurrences"
```

This is useful for verifying the tool's logic before letting the model call it.

### Testing a tool through the model

```swift
let text = SampleText.meetingNotes

let session = LanguageModelSession(
    tools: [
        CountPatternTool(text: text),
    ],
    instructions: Instructions {
        "You are a text analysis assistant."
        "Use the countPattern tool to answer questions about the text."
    }
)

let response = try await session.respond(to: "How many TODOs are in this document?")
print(response.content)
```

Run this and observe: Did the model call the tool? Did it format the response well? You can inspect the session transcript for details:

```swift
for entry in session.transcript.entries {
    print(entry)
}
```

### Testing multiple tools together

This is where the playground shines — you can test how the model chooses between tools:

```swift
struct GetInfoTool: Tool {
    let name = "getInfo"
    let description = "Get document statistics: word count, line count, character count."
    let text: String

    @Generable struct Arguments {}

    func call(arguments: Arguments) async throws -> String {
        let words = text.split(whereSeparator: \.isWhitespace).count
        let lines = text.components(separatedBy: .newlines).count
        let chars = text.count
        return "Words: \(words), Lines: \(lines), Characters: \(chars)"
    }
}

struct FindTool: Tool {
    let name = "findInDocument"
    let description = "Search the text for a word or phrase. Returns matching lines with line numbers."
    let text: String

    @Generable
    struct Arguments {
        @Guide(description: "The text to search for")
        var query: String
    }

    func call(arguments: Arguments) async throws -> String {
        let lines = text.components(separatedBy: .newlines)
        var results: [String] = []
        for (i, line) in lines.enumerated() {
            if line.localizedCaseInsensitiveContains(arguments.query) {
                results.append("Line \(i + 1): \(line)")
            }
        }
        if results.isEmpty { return "No matches found." }
        return results.prefix(10).joined(separator: "\n")
    }
}

let text = SampleText.meetingNotes

let session = LanguageModelSession(
    tools: [
        GetInfoTool(text: text),
        FindTool(text: text),
        CountPatternTool(text: text),
    ],
    instructions: Instructions {
        "You are a text editing assistant."
        "Use the provided tools to answer questions about the document."
        "Be concise."
    }
)

// Try different prompts and see which tools the model picks:
let r1 = try await session.respond(to: "How long is this document?")
print("Q1:", r1.content)

// New session for independent test (avoids filling context)
let session2 = LanguageModelSession(
    tools: [GetInfoTool(text: text), FindTool(text: text), CountPatternTool(text: text)],
    instructions: Instructions { "You are a text editing assistant. Be concise." }
)
let r2 = try await session2.respond(to: "Find all the action items")
print("Q2:", r2.content)
```

### Testing mutation tools

For tools that modify text, use a class wrapper so you can see the before/after:

```swift
class MutableText {
    var content: String
    init(_ content: String) { self.content = content }
}

struct ReplaceInDocumentTool: Tool {
    let name = "replaceInDocument"
    let description = "Find and replace text in the document."
    let doc: MutableText

    @Generable
    struct Arguments {
        @Guide(description: "The text to find")
        var search: String
        @Guide(description: "The replacement text")
        var replacement: String
        @Guide(description: "Replace all occurrences (true) or just the first (false)")
        var all: Bool
    }

    func call(arguments: Arguments) async throws -> String {
        if arguments.all {
            let count = doc.content.components(separatedBy: arguments.search).count - 1
            doc.content = doc.content.replacingOccurrences(of: arguments.search, with: arguments.replacement)
            return "Replaced \(count) occurrence(s)."
        } else {
            if let range = doc.content.range(of: arguments.search) {
                doc.content = doc.content.replacingCharacters(in: range, with: arguments.replacement)
                return "Replaced 1 occurrence."
            }
            return "No match found."
        }
    }
}

let doc = MutableText(SampleText.meetingNotes)
print("Before:", doc.content.prefix(100))

let session = LanguageModelSession(
    tools: [ReplaceInDocumentTool(doc: doc)],
    instructions: Instructions {
        "You are a text editing assistant. When asked to change the document, use the replaceInDocument tool."
    }
)

let response = try await session.respond(to: "Change every TODO to DONE")
print("Response:", response.content)
print("After:", doc.content.prefix(200))
```

## Experimenting with instructions

The `Instructions` you give the session control how the model behaves. The playground is the place to tune them.

### Things to test

**Tool selection accuracy**: Does the model pick the right tool?
```swift
// Try ambiguous prompts and see what happens:
"Tell me about this document"       // Should it call getInfo? findInDocument? Both?
"Are there any TODOs?"              // countPattern or findInDocument?
"How many words?"                   // getInfo (not countPattern)
```

**Instruction specificity**: How explicit do the instructions need to be?
```swift
// Vague instructions — does the model still work?
Instructions { "Help the user with their document." }

// Very specific — does this improve tool selection?
Instructions {
    "You are a text editing assistant for a plain text document."
    "Use getInfo for questions about document size (words, lines, characters)."
    "Use findInDocument for questions about what the document contains."
    "Use countPattern for questions about how many times something appears."
    "Be concise. Do not reproduce large amounts of text."
}
```

**Response verbosity**: Can you get the model to be terse?
```swift
// Add this to instructions and see if it helps:
"Reply in one sentence."
"Do not echo tool output. Summarize it."
```

### Watching token usage

Keep an eye on context consumption. After a few turns, try:

```swift
// If this throws exceededContextWindowSize, your tools return too much text
do {
    let response = try await session.respond(to: "one more question")
} catch {
    print("Context full after N turns: \(error)")
}
```

This tells you how many conversational turns your instruction + tool schema budget allows.

## Playground pages

As experiments grow, organize them into pages (File > New > Playground Page):

```
ToolExperiments.playground/
  Sources/
    SampleText.swift           # Shared test documents
  Pages/
    Inspection Tools.xcplaygroundpage    # getInfo, find, count
    Mutation Tools.xcplaygroundpage      # replace, insert, wrap
    Composed Features.xcplaygroundpage   # multi-tool prompts
    Instruction Tuning.xcplaygroundpage  # prompt wording experiments
```

Each page runs independently. You can iterate on mutation tools without re-running inspection tool experiments.

## Moving to the app

When a tool definition works well in the playground:

1. Move the `Tool` struct into the app target (or LilDocKit once it exists)
2. Replace `let text: String` / `let doc: MutableText` with `DocumentAccess` from the on-device agent plan
3. The `@Generable` arguments, tool name, description, and `call` logic transfer unchanged
4. Delete the playground copy

The playground is disposable scaffolding. The tools are the product.

## Gotchas

- **Playgrounds and async**: Playground pages support top-level `await`. If you get errors about async context, wrap code in a `Task { }` block.
- **Playgrounds and macros**: `@Generable` and `@Guide` require the Swift compiler to expand macros. If you get macro expansion errors, make sure you're running Xcode 26+ with the macOS 26 SDK.
- **One session at a time**: `LanguageModelSession` only handles one `respond` call at a time. Don't `await` two responses concurrently on the same session.
- **Playground Sources and @Generable**: If you put `@Generable` structs in `Sources/`, they compile as a separate module. The Foundation Models framework should still see them, but if you hit issues, keep `@Generable` types on the playground page itself.
- **Rate limiting**: The on-device model may throttle rapid successive calls. If the playground hangs, wait a moment and re-run.
